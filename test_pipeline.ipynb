{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install duckdb pandas numpy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import duckdb\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ETL Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Olist Dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lewati jika sudah ada cache pickle\n",
        "\n",
        "import kagglehub as kh\n",
        "import os\n",
        "\n",
        "if not os.path.exists('dataset_dict.pkl'):\n",
        "    path = kh.dataset_download(\"olistbr/brazilian-ecommerce\")\n",
        "\n",
        "    print(\"Path to dataset files:\", path)\n",
        "\n",
        "    customers_csv = pd.read_csv(os.path.join(path,'olist_customers_dataset.csv'))\n",
        "    geo_csv = pd.read_csv(os.path.join(path,'olist_geolocation_dataset.csv'))\n",
        "    items_csv = pd.read_csv(os.path.join(path,'olist_order_items_dataset.csv'))\n",
        "    payments_csv = pd.read_csv(os.path.join(path,'olist_order_payments_dataset.csv'))\n",
        "    reviews_csv = pd.read_csv(os.path.join(path,'olist_order_reviews_dataset.csv'))\n",
        "    orders_csv = pd.read_csv(os.path.join(path,'olist_orders_dataset.csv'))\n",
        "    products_csv = pd.read_csv(os.path.join(path,'olist_products_dataset.csv'))\n",
        "    sellers_csv = pd.read_csv(os.path.join(path,'olist_sellers_dataset.csv'))\n",
        "    category_csv = pd.read_csv(os.path.join(path,'product_category_name_translation.csv'))\n",
        "\n",
        "    dataset_dict = {\n",
        "      'customers': customers_csv,\n",
        "      'geo': geo_csv,\n",
        "      'items': items_csv,\n",
        "      'payments': payments_csv,\n",
        "      'reviews': reviews_csv,\n",
        "      'orders': orders_csv,\n",
        "      'products': products_csv,\n",
        "      'sellers': sellers_csv,\n",
        "      'category': category_csv\n",
        "    }\n",
        "\n",
        "    # Save the dataset_dict to a pickle file\n",
        "    with open('dataset_dict.pkl', 'wb') as f:\n",
        "        pickle.dump(dataset_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Olist Dataset from Cache (Pickle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['customers', 'geo', 'items', 'payments', 'reviews', 'orders', 'products', 'sellers', 'category'])\n"
          ]
        }
      ],
      "source": [
        "# Load from cache (the pickle file)\n",
        "with open('dataset_dict.pkl', 'rb') as f:\n",
        "    data_from_pkl = pickle.load(f)\n",
        "\n",
        "print(data_from_pkl.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ETLpipeline:\n",
        "    def __init__(self, db=\"brazilian_ecommerce.db\"):\n",
        "        self.conn = duckdb.connect(database=db, read_only=False)\n",
        "        self.initialized = False\n",
        "        self.log_table_setup()\n",
        "\n",
        "    def log_table_setup(self):\n",
        "        \"\"\"Membuat tabel log untuk melacak proses ETL\"\"\"\n",
        "        self.conn.execute(\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS etl_log (\n",
        "            log_id INTEGER PRIMARY KEY,\n",
        "            process_name VARCHAR,\n",
        "            start_time TIMESTAMP,\n",
        "            end_time TIMESTAMP,\n",
        "            records_processed INTEGER,\n",
        "            status VARCHAR,\n",
        "            message VARCHAR\n",
        "        )\n",
        "        \"\"\")\n",
        "\n",
        "    def log_process_start(self, process_name):\n",
        "        \"\"\"Mencatat mulainya proses ETL\"\"\"\n",
        "        log_id = self.conn.execute(\"SELECT COALESCE(MAX(log_id), 0) + 1 FROM etl_log\").fetchone()[0]\n",
        "        self.conn.execute(\"\"\"\n",
        "        INSERT INTO etl_log (log_id, process_name, start_time, status)\n",
        "        VALUES (?, ?, CURRENT_TIMESTAMP, 'RUNNING')\n",
        "        \"\"\", [log_id, process_name])\n",
        "        return log_id\n",
        "\n",
        "    def log_process_end(self, log_id, records=0, status='SUCCESS', message=None):\n",
        "        \"\"\"Mencatat selesainya proses ETL\"\"\"\n",
        "        self.conn.execute(\"\"\"\n",
        "        UPDATE etl_log\n",
        "        SET end_time = CURRENT_TIMESTAMP,\n",
        "            records_processed = ?,\n",
        "            status = ?,\n",
        "            message = ?\n",
        "        WHERE log_id = ?\n",
        "        \"\"\", [records, status, message, log_id])\n",
        "\n",
        "    def initialize_warehouse_schema(self):\n",
        "        if self.initialized:\n",
        "            return\n",
        "\n",
        "        log_id = self.log_process_start(\"initialize_warehouse_schema\")\n",
        "        try:\n",
        "            self.conn.execute(\"DROP TABLE IF EXISTS dim_payments\")\n",
        "            self.conn.execute(\"DROP TABLE IF EXISTS fact_order_items\")\n",
        "            self.conn.execute(\"DROP TABLE IF EXISTS fact_orders\")\n",
        "            self.conn.execute(\"DROP TABLE IF EXISTS dim_products\")\n",
        "            self.conn.execute(\"DROP TABLE IF EXISTS dim_sellers\")\n",
        "            self.conn.execute(\"DROP TABLE IF EXISTS dim_customers\")\n",
        "            self.conn.execute(\"DROP TABLE IF EXISTS dim_date\")\n",
        "            self.conn.execute(\"DROP TABLE IF EXISTS dim_category\")\n",
        "\n",
        "            self.conn.execute(\"\"\"\n",
        "            CREATE OR REPLACE TABLE dim_date AS\n",
        "            WITH date_range AS (\n",
        "                SELECT unnest(generate_series('2016-09-01'::DATE, '2020-05-01'::DATE, INTERVAL '1 day')) as date\n",
        "            )\n",
        "            SELECT\n",
        "                (EXTRACT(YEAR FROM date) * 10000 + EXTRACT(MONTH FROM date) * 100 + EXTRACT(DAY FROM date))::INTEGER AS date_id,\n",
        "                date as date_value,\n",
        "                EXTRACT(DAY FROM date) AS day,\n",
        "                EXTRACT(MONTH FROM date) AS month,\n",
        "                strftime(date, '%B') AS month_name,\n",
        "                EXTRACT(YEAR FROM date) AS year,\n",
        "                strftime(date, '%A') AS day_name,\n",
        "                EXTRACT(DOW FROM date) AS day_of_week,\n",
        "                EXTRACT(QUARTER FROM date) AS quarter\n",
        "            FROM date_range;\n",
        "            \"\"\")\n",
        "\n",
        "            # Add primary key to dim_date table\n",
        "            self.conn.execute(\"ALTER TABLE dim_date ADD PRIMARY KEY (date_id);\")\n",
        "\n",
        "            self.conn.execute(\"\"\"\n",
        "            -- Tabel Dimensi\n",
        "            CREATE OR REPLACE TABLE dim_customers (\n",
        "                customer_id VARCHAR(50) PRIMARY KEY,\n",
        "                customer_unique_id VARCHAR(50) NOT NULL,\n",
        "                customer_city VARCHAR(100),\n",
        "                customer_state VARCHAR(50),\n",
        "                customer_zip_code_prefix VARCHAR(20)\n",
        "            );\n",
        "\n",
        "            CREATE OR REPLACE TABLE dim_sellers (\n",
        "                seller_id VARCHAR(50) PRIMARY KEY,\n",
        "                seller_zip_code_prefix VARCHAR(20),\n",
        "                seller_city VARCHAR(100),\n",
        "                seller_state VARCHAR(50)\n",
        "            );\n",
        "\n",
        "            -- scd type 2\n",
        "            CREATE OR REPLACE TABLE dim_products (\n",
        "                produk_key INT PRIMARY KEY,\n",
        "                product_id VARCHAR(50) UNIQUE NOT NULL,\n",
        "                product_category_name VARCHAR(100),\n",
        "                product_category_name_english VARCHAR(100),\n",
        "                product_weight_g DECIMAL(10,2),\n",
        "                product_length_cm DECIMAL(10,2),\n",
        "                product_height_cm DECIMAL(10,2),\n",
        "                product_width_cm DECIMAL(10,2),\n",
        "                effective_date DATE NOT NULL,\n",
        "                expiration_date DATE,\n",
        "                current_flag BOOLEAN DEFAULT TRUE\n",
        "            );\n",
        "\n",
        "            -- Tabel Fakta Pemesanan\n",
        "            CREATE OR REPLACE TABLE fact_orders (\n",
        "                order_id VARCHAR(50) PRIMARY KEY,\n",
        "                customer_id VARCHAR(50) NOT NULL,\n",
        "                date_id INT NOT NULL,\n",
        "                order_status VARCHAR(50),\n",
        "                order_purchase_timestamp DATETIME NOT NULL,\n",
        "                order_approved_at DATETIME,\n",
        "                order_delivered_customer_date DATETIME,\n",
        "                order_estimated_delivery_date DATETIME,\n",
        "                sales_amount DECIMAL(10,2),\n",
        "                FOREIGN KEY (customer_id) REFERENCES dim_customers(customer_id),\n",
        "                FOREIGN KEY (date_id) REFERENCES dim_date(date_id)\n",
        "            );\n",
        "\n",
        "            -- Tabel Fakta Item Pemesanan\n",
        "            CREATE OR REPLACE TABLE fact_order_items (\n",
        "                order_id VARCHAR(50) NOT NULL,\n",
        "                order_item_id VARCHAR(50) NOT NULL,\n",
        "                product_id VARCHAR(50) NOT NULL,\n",
        "                seller_id VARCHAR(50) NOT NULL,\n",
        "                date_id INT NOT NULL,\n",
        "                shipping_limit_date DATETIME,\n",
        "                price DECIMAL(10,2),\n",
        "                freight_value DECIMAL(10,2),\n",
        "                PRIMARY KEY (order_id, order_item_id),\n",
        "                FOREIGN KEY (order_id) REFERENCES fact_orders(order_id),\n",
        "                FOREIGN KEY (seller_id) REFERENCES dim_sellers(seller_id),\n",
        "                FOREIGN KEY (product_id) REFERENCES dim_products(product_id),\n",
        "                FOREIGN KEY (date_id) REFERENCES dim_date(date_id)\n",
        "            );\n",
        "\n",
        "            -- DuckDB belum mendukung ALTER TABLE ADD FOREIGN KEY\n",
        "            -- Sehingga dimensi payments harus setelah fact_orders karena foreign key\n",
        "            CREATE OR REPLACE TABLE dim_payments (\n",
        "                payment_id INT PRIMARY KEY,\n",
        "                order_id VARCHAR(50) NOT NULL,\n",
        "                payment_sequential INT,\n",
        "                payment_type VARCHAR(50),\n",
        "                payment_installments INT,\n",
        "                payment_value DECIMAL(10,2),\n",
        "                FOREIGN KEY (order_id) REFERENCES fact_orders(order_id)\n",
        "            );\n",
        "            \"\"\")\n",
        "\n",
        "            self.initialized = True\n",
        "            self.log_process_end(log_id, status='SUCCESS', message='Data warehouse schema initialized')\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log_process_end(log_id, status='ERROR', message=str(e))\n",
        "            raise\n",
        "\n",
        "\n",
        "    def extract(self):\n",
        "        log_id = self.log_process_start(\"extract\")\n",
        "\n",
        "        customers_df = data_from_pkl['customers']\n",
        "        sellers_df = data_from_pkl['sellers']\n",
        "        products_df = data_from_pkl['products']\n",
        "        category_df = data_from_pkl['category']\n",
        "        payments_df = data_from_pkl['payments']\n",
        "        orders_df = data_from_pkl['orders']\n",
        "        items_df = data_from_pkl['items']\n",
        "\n",
        "        # Extracting data from packle file\n",
        "        try:\n",
        "            # Menggunakan staging table dari DuckDB\n",
        "            self.conn.execute(\"\"\"\n",
        "                CREATE OR REPLACE TABLE stg_customers AS SELECT * FROM customers_df;\n",
        "                CREATE OR REPLACE TABLE stg_sellers AS SELECT * FROM sellers_df;\n",
        "                CREATE OR REPLACE TABLE stg_products AS SELECT * FROM products_df;\n",
        "                CREATE OR REPLACE TABLE stg_payments AS SELECT * FROM payments_df;\n",
        "                CREATE OR REPLACE TABLE stg_orders AS SELECT * FROM orders_df;\n",
        "                CREATE OR REPLACE TABLE stg_items AS SELECT * FROM items_df;\n",
        "                CREATE OR REPLACE TABLE stg_category AS SELECT * FROM category_df;\n",
        "            \"\"\")\n",
        "\n",
        "            # Logging\n",
        "            total_records = 0\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_customers\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_sellers\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_products\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_payments\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_orders\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_items\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_category\").fetchone()[0]\n",
        "\n",
        "            self.log_process_end(log_id, records=total_records, status='SUCCESS')\n",
        "            # print(f\"Extract total records: {total_records}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log_process_end(log_id, status='ERROR', message=str(e))\n",
        "            raise\n",
        "\n",
        "    def transform(self):\n",
        "        log_id = self.log_process_start(\"transform\")\n",
        "\n",
        "        try:\n",
        "            # Dimensi Customers\n",
        "            stg_customers = self.conn.execute(\"SELECT * FROM stg_customers\").fetchdf()\n",
        "            stg_customers = stg_customers.drop_duplicates(subset=['customer_id'])\n",
        "            stg_customers_sorted = stg_customers[['customer_id', 'customer_unique_id', 'customer_city', 'customer_state', 'customer_zip_code_prefix']]\n",
        "            self.conn.execute(\"CREATE OR REPLACE TABLE stg_dim_customers AS SELECT * FROM stg_customers_sorted\")\n",
        "\n",
        "            # Dimensi Sellers\n",
        "            stg_sellers = self.conn.execute(\"SELECT * FROM stg_sellers\").fetchdf()\n",
        "            stg_sellers = stg_sellers.drop_duplicates(subset=['seller_id'])\n",
        "            stg_sellers_sorted = stg_sellers[['seller_id', 'seller_city', 'seller_state', 'seller_zip_code_prefix']]\n",
        "            self.conn.execute(\"CREATE OR REPLACE TABLE stg_dim_sellers AS SELECT * FROM stg_sellers_sorted\")\n",
        "\n",
        "            # Dimensi Products\n",
        "            stg_products = self.conn.execute(\"SELECT * FROM stg_products\").fetchdf()\n",
        "            stg_products = stg_products.drop_duplicates(subset=['product_id'])\n",
        "\n",
        "            stg_products = stg_products.drop('product_name_lenght', axis=1)\n",
        "            stg_products = stg_products.drop('product_description_lenght', axis=1)\n",
        "            stg_products = stg_products.drop('product_photos_qty', axis=1)\n",
        "\n",
        "            for column in ['product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']:\n",
        "                mean_value = stg_products[column].mean()\n",
        "                stg_products[column] = stg_products[column].fillna(mean_value)\n",
        "\n",
        "            stg_products['product_key'] = range(1, len(stg_products) + 1)\n",
        "            stg_products['effective_date'] = datetime.now().date()\n",
        "            stg_products['expiration_date'] = None\n",
        "            stg_products['current_flag'] = True\n",
        "\n",
        "            stg_category = self.conn.execute(\"SELECT * FROM stg_category\").fetchdf()\n",
        "            for i in range(len(stg_category)):\n",
        "                ctg = stg_category['product_category_name'][i]\n",
        "                eng_ctg = stg_category['product_category_name_english'][i]\n",
        "                if (ctg in stg_products['product_category_name'].unique()):\n",
        "                    stg_products.loc[stg_products['product_category_name'] == ctg, 'product_category_name_english'] = eng_ctg\n",
        "\n",
        "            stg_products_sorted = stg_products[['product_key', 'product_id',\n",
        "                                                'product_category_name', 'product_category_name_english',\n",
        "                                                'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm',\n",
        "                                                'effective_date', 'expiration_date', 'current_flag']]\n",
        "            self.conn.execute(\"CREATE OR REPLACE TABLE stg_dim_products AS SELECT * FROM stg_products_sorted\")\n",
        "\n",
        "            # Dimensi Payments\n",
        "            stg_payments = self.conn.execute(\"SELECT * FROM stg_payments\").fetchdf()\n",
        "            stg_payments['payment_id'] = range(1, len(stg_payments) + 1)\n",
        "            stg_payments_sorted = stg_payments[['payment_id', 'order_id', 'payment_sequential', 'payment_type', 'payment_installments', 'payment_value']]\n",
        "            self.conn.execute(\"CREATE OR REPLACE TABLE stg_dim_payments AS SELECT * FROM stg_payments_sorted\")\n",
        "\n",
        "            # Fakta Orders\n",
        "            stg_orders = self.conn.execute(\"SELECT * FROM stg_orders\").fetchdf()\n",
        "            stg_orders = stg_orders.drop('order_delivered_carrier_date', axis=1)\n",
        "\n",
        "            stg_orders['date_id'] = pd.to_datetime(stg_orders['order_purchase_timestamp']).dt.strftime('%Y%m%d').astype(int)\n",
        "\n",
        "            orders_cols = list(stg_orders.columns)\n",
        "            customer_id_index = orders_cols.index('customer_id')\n",
        "            orders_cols.insert(customer_id_index + 1, orders_cols.pop(orders_cols.index('date_id')))\n",
        "            stg_orders_sorted = stg_orders[orders_cols]\n",
        "\n",
        "            stg_items = self.conn.execute(\"SELECT * FROM stg_items\").fetchdf()\n",
        "            sales_amount_df = stg_items.groupby('order_id')['price'].sum().reset_index()\n",
        "            sales_amount_df = sales_amount_df.rename(columns={'price': 'sales_amount'})\n",
        "            stg_orders_sorted = pd.merge(stg_orders_sorted, sales_amount_df, on='order_id', how='left')\n",
        "            self.conn.execute(\"CREATE OR REPLACE TABLE stg_fact_orders AS SELECT * FROM stg_orders_sorted\")\n",
        "\n",
        "            # Fakta Order Items\n",
        "            stg_order_items = self.conn.execute(\"SELECT * FROM stg_items\").fetchdf()\n",
        "            stg_order_items['date_id'] = pd.to_datetime(stg_order_items['shipping_limit_date']).dt.strftime('%Y%m%d').astype(int)\n",
        "\n",
        "            items_cols = list(stg_order_items.columns)\n",
        "            product_id_index = items_cols.index('seller_id')\n",
        "            items_cols.insert(product_id_index + 1, items_cols.pop(items_cols.index('date_id')))\n",
        "            stg_order_items_sorted = stg_order_items[items_cols]\n",
        "            self.conn.execute(\"CREATE OR REPLACE TABLE stg_fact_order_items AS SELECT * FROM stg_order_items_sorted\")\n",
        "\n",
        "            # Logging\n",
        "            total_records = 0\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_dim_customers\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_dim_sellers\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_dim_products\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_dim_payments\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_fact_orders\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM stg_fact_order_items\").fetchone()[0]\n",
        "            self.log_process_end(log_id, records=total_records, status='SUCCESS')\n",
        "            # print(f\"Transform total records: {total_records}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log_process_end(log_id, status='ERROR', message=str(e))\n",
        "            raise\n",
        "\n",
        "    def load(self):\n",
        "        # Loading data into DuckDB\n",
        "        log_id = self.log_process_start(\"load\")\n",
        "        try:\n",
        "            stg_dim_customers = self.conn.execute(\"SELECT * FROM stg_dim_customers\").fetchdf()\n",
        "            stg_dim_sellers = self.conn.execute(\"SELECT * FROM stg_dim_sellers\").fetchdf()\n",
        "            stg_dim_products = self.conn.execute(\"SELECT * FROM stg_dim_products\").fetchdf()\n",
        "            stg_dim_payments = self.conn.execute(\"SELECT * FROM stg_dim_payments\").fetchdf()\n",
        "            stg_fact_orders = self.conn.execute(\"SELECT * FROM stg_fact_orders\").fetchdf()\n",
        "            stg_fact_order_items = self.conn.execute(\"SELECT * FROM stg_fact_order_items\").fetchdf()\n",
        "\n",
        "            self.conn.execute(\"INSERT INTO dim_customers SELECT * FROM stg_dim_customers\")\n",
        "            self.conn.execute(\"INSERT INTO dim_sellers SELECT * FROM stg_dim_sellers\")\n",
        "            self.conn.execute(\"INSERT INTO dim_products SELECT * FROM stg_dim_products\")\n",
        "            self.conn.execute(\"INSERT INTO fact_orders SELECT * FROM stg_fact_orders\")\n",
        "            self.conn.execute(\"INSERT INTO fact_order_items SELECT * FROM stg_fact_order_items\")\n",
        "            self.conn.execute(\"INSERT INTO dim_payments SELECT * FROM stg_dim_payments\")\n",
        "\n",
        "            total_records = 0\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM dim_customers\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM dim_sellers\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM dim_products\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM dim_payments\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM fact_orders\").fetchone()[0]\n",
        "            total_records += self.conn.execute(\"SELECT COUNT(*) FROM fact_order_items\").fetchone()[0]\n",
        "\n",
        "            self.log_process_end(log_id, records=total_records, status='SUCCESS')\n",
        "            # print(f\"Load total records: {total_records}\")\n",
        "\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log_process_end(log_id, status='ERROR', message=str(e))\n",
        "            raise\n",
        "\n",
        "    def run_pipeline(self):\n",
        "        self.initialize_warehouse_schema()\n",
        "        self.extract()\n",
        "        self.transform()\n",
        "        self.load()\n",
        "\n",
        "    def get_etl_log(self, limit=10):\n",
        "        return self.conn.execute(f\"\"\"\n",
        "            SELECT\n",
        "                log_id,\n",
        "                process_name,\n",
        "                start_time,\n",
        "                end_time,\n",
        "                EXTRACT(EPOCH FROM (end_time - start_time)) as duration_seconds,\n",
        "                records_processed,\n",
        "                status,\n",
        "                message\n",
        "            FROM etl_log\n",
        "            ORDER BY log_id DESC\n",
        "            LIMIT {limit}\n",
        "        \"\"\").fetchdf()\n",
        "    \n",
        "    def close_connection(self):\n",
        "        try:\n",
        "            if hasattr(self, 'conn'):\n",
        "                self.conn.close()\n",
        "                print(\"Koneksi DuckDB ditutup.\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "etl = ETLpipeline()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run ETL Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>log_id</th>\n",
              "      <th>process_name</th>\n",
              "      <th>start_time</th>\n",
              "      <th>end_time</th>\n",
              "      <th>duration_seconds</th>\n",
              "      <th>records_processed</th>\n",
              "      <th>status</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>load</td>\n",
              "      <td>2025-04-03 21:08:34.528</td>\n",
              "      <td>2025-04-03 21:08:46.449</td>\n",
              "      <td>11.921</td>\n",
              "      <td>451464</td>\n",
              "      <td>SUCCESS</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>transform</td>\n",
              "      <td>2025-04-03 21:08:27.595</td>\n",
              "      <td>2025-04-03 21:08:34.370</td>\n",
              "      <td>6.775</td>\n",
              "      <td>451464</td>\n",
              "      <td>SUCCESS</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>extract</td>\n",
              "      <td>2025-04-03 21:08:23.424</td>\n",
              "      <td>2025-04-03 21:08:27.588</td>\n",
              "      <td>4.164</td>\n",
              "      <td>451535</td>\n",
              "      <td>SUCCESS</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>initialize_warehouse_schema</td>\n",
              "      <td>2025-04-03 21:08:23.268</td>\n",
              "      <td>2025-04-03 21:08:23.409</td>\n",
              "      <td>0.141</td>\n",
              "      <td>0</td>\n",
              "      <td>SUCCESS</td>\n",
              "      <td>Data warehouse schema initialized</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>load</td>\n",
              "      <td>2025-04-03 20:59:12.496</td>\n",
              "      <td>2025-04-03 20:59:25.911</td>\n",
              "      <td>13.415</td>\n",
              "      <td>451464</td>\n",
              "      <td>SUCCESS</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>transform</td>\n",
              "      <td>2025-04-03 20:59:04.654</td>\n",
              "      <td>2025-04-03 20:59:12.350</td>\n",
              "      <td>7.696</td>\n",
              "      <td>451464</td>\n",
              "      <td>SUCCESS</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2</td>\n",
              "      <td>extract</td>\n",
              "      <td>2025-04-03 20:59:01.791</td>\n",
              "      <td>2025-04-03 20:59:04.640</td>\n",
              "      <td>2.849</td>\n",
              "      <td>451535</td>\n",
              "      <td>SUCCESS</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>initialize_warehouse_schema</td>\n",
              "      <td>2025-04-03 20:59:01.687</td>\n",
              "      <td>2025-04-03 20:59:01.784</td>\n",
              "      <td>0.097</td>\n",
              "      <td>0</td>\n",
              "      <td>SUCCESS</td>\n",
              "      <td>Data warehouse schema initialized</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   log_id                 process_name              start_time  \\\n",
              "0       8                         load 2025-04-03 21:08:34.528   \n",
              "1       7                    transform 2025-04-03 21:08:27.595   \n",
              "2       6                      extract 2025-04-03 21:08:23.424   \n",
              "3       5  initialize_warehouse_schema 2025-04-03 21:08:23.268   \n",
              "4       4                         load 2025-04-03 20:59:12.496   \n",
              "5       3                    transform 2025-04-03 20:59:04.654   \n",
              "6       2                      extract 2025-04-03 20:59:01.791   \n",
              "7       1  initialize_warehouse_schema 2025-04-03 20:59:01.687   \n",
              "\n",
              "                 end_time  duration_seconds  records_processed   status  \\\n",
              "0 2025-04-03 21:08:46.449            11.921             451464  SUCCESS   \n",
              "1 2025-04-03 21:08:34.370             6.775             451464  SUCCESS   \n",
              "2 2025-04-03 21:08:27.588             4.164             451535  SUCCESS   \n",
              "3 2025-04-03 21:08:23.409             0.141                  0  SUCCESS   \n",
              "4 2025-04-03 20:59:25.911            13.415             451464  SUCCESS   \n",
              "5 2025-04-03 20:59:12.350             7.696             451464  SUCCESS   \n",
              "6 2025-04-03 20:59:04.640             2.849             451535  SUCCESS   \n",
              "7 2025-04-03 20:59:01.784             0.097                  0  SUCCESS   \n",
              "\n",
              "                             message  \n",
              "0                               None  \n",
              "1                               None  \n",
              "2                               None  \n",
              "3  Data warehouse schema initialized  \n",
              "4                               None  \n",
              "5                               None  \n",
              "6                               None  \n",
              "7  Data warehouse schema initialized  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "etl.run_pipeline()\n",
        "etl.get_etl_log()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Koneksi DuckDB ditutup.\n"
          ]
        }
      ],
      "source": [
        "etl.close_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>order_id</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>date_id</th>\n",
              "      <th>order_status</th>\n",
              "      <th>order_purchase_timestamp</th>\n",
              "      <th>order_approved_at</th>\n",
              "      <th>order_delivered_customer_date</th>\n",
              "      <th>order_estimated_delivery_date</th>\n",
              "      <th>sales_amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
              "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
              "      <td>20171002</td>\n",
              "      <td>delivered</td>\n",
              "      <td>2017-10-02 10:56:33</td>\n",
              "      <td>2017-10-02 11:07:15</td>\n",
              "      <td>2017-10-10 21:25:13</td>\n",
              "      <td>2017-10-18</td>\n",
              "      <td>29.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
              "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
              "      <td>20180724</td>\n",
              "      <td>delivered</td>\n",
              "      <td>2018-07-24 20:41:37</td>\n",
              "      <td>2018-07-26 03:24:27</td>\n",
              "      <td>2018-08-07 15:27:45</td>\n",
              "      <td>2018-08-13</td>\n",
              "      <td>118.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
              "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
              "      <td>20180808</td>\n",
              "      <td>delivered</td>\n",
              "      <td>2018-08-08 08:38:49</td>\n",
              "      <td>2018-08-08 08:55:23</td>\n",
              "      <td>2018-08-17 18:06:29</td>\n",
              "      <td>2018-09-04</td>\n",
              "      <td>159.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>949d5b44dbf5de918fe9c16f97b45f8a</td>\n",
              "      <td>f88197465ea7920adcdbec7375364d82</td>\n",
              "      <td>20171118</td>\n",
              "      <td>delivered</td>\n",
              "      <td>2017-11-18 19:28:06</td>\n",
              "      <td>2017-11-18 19:45:59</td>\n",
              "      <td>2017-12-02 00:28:42</td>\n",
              "      <td>2017-12-15</td>\n",
              "      <td>45.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ad21c59c0840e6cb83a9ceb5573f8159</td>\n",
              "      <td>8ab97904e6daea8866dbdbc4fb7aad2c</td>\n",
              "      <td>20180213</td>\n",
              "      <td>delivered</td>\n",
              "      <td>2018-02-13 21:18:39</td>\n",
              "      <td>2018-02-13 22:20:29</td>\n",
              "      <td>2018-02-16 18:17:02</td>\n",
              "      <td>2018-02-26</td>\n",
              "      <td>19.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99436</th>\n",
              "      <td>9c5dedf39a927c1b2549525ed64a053c</td>\n",
              "      <td>39bd1228ee8140590ac3aca26f2dfe00</td>\n",
              "      <td>20170309</td>\n",
              "      <td>delivered</td>\n",
              "      <td>2017-03-09 09:54:05</td>\n",
              "      <td>2017-03-09 09:54:05</td>\n",
              "      <td>2017-03-17 15:08:01</td>\n",
              "      <td>2017-03-28</td>\n",
              "      <td>72.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99437</th>\n",
              "      <td>63943bddc261676b46f01ca7ac2f7bd8</td>\n",
              "      <td>1fca14ff2861355f6e5f14306ff977a7</td>\n",
              "      <td>20180206</td>\n",
              "      <td>delivered</td>\n",
              "      <td>2018-02-06 12:58:58</td>\n",
              "      <td>2018-02-06 13:10:37</td>\n",
              "      <td>2018-02-28 17:37:56</td>\n",
              "      <td>2018-03-02</td>\n",
              "      <td>174.90</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99438</th>\n",
              "      <td>83c1379a015df1e13d02aae0204711ab</td>\n",
              "      <td>1aa71eb042121263aafbe80c1b562c9c</td>\n",
              "      <td>20170827</td>\n",
              "      <td>delivered</td>\n",
              "      <td>2017-08-27 14:46:43</td>\n",
              "      <td>2017-08-27 15:04:16</td>\n",
              "      <td>2017-09-21 11:24:17</td>\n",
              "      <td>2017-09-27</td>\n",
              "      <td>205.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99439</th>\n",
              "      <td>11c177c8e97725db2631073c19f07b62</td>\n",
              "      <td>b331b74b18dc79bcdf6532d51e1637c1</td>\n",
              "      <td>20180108</td>\n",
              "      <td>delivered</td>\n",
              "      <td>2018-01-08 21:28:27</td>\n",
              "      <td>2018-01-08 21:36:21</td>\n",
              "      <td>2018-01-25 23:32:54</td>\n",
              "      <td>2018-02-15</td>\n",
              "      <td>359.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99440</th>\n",
              "      <td>66dea50a8b16d9b4dee7af250b4be1a5</td>\n",
              "      <td>edb027a75a1449115f6b43211ae02a24</td>\n",
              "      <td>20180308</td>\n",
              "      <td>delivered</td>\n",
              "      <td>2018-03-08 20:57:30</td>\n",
              "      <td>2018-03-09 11:20:28</td>\n",
              "      <td>2018-03-16 13:08:30</td>\n",
              "      <td>2018-04-03</td>\n",
              "      <td>68.50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>99441 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               order_id                       customer_id  \\\n",
              "0      e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
              "1      53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
              "2      47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
              "3      949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
              "4      ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
              "...                                 ...                               ...   \n",
              "99436  9c5dedf39a927c1b2549525ed64a053c  39bd1228ee8140590ac3aca26f2dfe00   \n",
              "99437  63943bddc261676b46f01ca7ac2f7bd8  1fca14ff2861355f6e5f14306ff977a7   \n",
              "99438  83c1379a015df1e13d02aae0204711ab  1aa71eb042121263aafbe80c1b562c9c   \n",
              "99439  11c177c8e97725db2631073c19f07b62  b331b74b18dc79bcdf6532d51e1637c1   \n",
              "99440  66dea50a8b16d9b4dee7af250b4be1a5  edb027a75a1449115f6b43211ae02a24   \n",
              "\n",
              "        date_id order_status order_purchase_timestamp   order_approved_at  \\\n",
              "0      20171002    delivered      2017-10-02 10:56:33 2017-10-02 11:07:15   \n",
              "1      20180724    delivered      2018-07-24 20:41:37 2018-07-26 03:24:27   \n",
              "2      20180808    delivered      2018-08-08 08:38:49 2018-08-08 08:55:23   \n",
              "3      20171118    delivered      2017-11-18 19:28:06 2017-11-18 19:45:59   \n",
              "4      20180213    delivered      2018-02-13 21:18:39 2018-02-13 22:20:29   \n",
              "...         ...          ...                      ...                 ...   \n",
              "99436  20170309    delivered      2017-03-09 09:54:05 2017-03-09 09:54:05   \n",
              "99437  20180206    delivered      2018-02-06 12:58:58 2018-02-06 13:10:37   \n",
              "99438  20170827    delivered      2017-08-27 14:46:43 2017-08-27 15:04:16   \n",
              "99439  20180108    delivered      2018-01-08 21:28:27 2018-01-08 21:36:21   \n",
              "99440  20180308    delivered      2018-03-08 20:57:30 2018-03-09 11:20:28   \n",
              "\n",
              "      order_delivered_customer_date order_estimated_delivery_date  \\\n",
              "0               2017-10-10 21:25:13                    2017-10-18   \n",
              "1               2018-08-07 15:27:45                    2018-08-13   \n",
              "2               2018-08-17 18:06:29                    2018-09-04   \n",
              "3               2017-12-02 00:28:42                    2017-12-15   \n",
              "4               2018-02-16 18:17:02                    2018-02-26   \n",
              "...                             ...                           ...   \n",
              "99436           2017-03-17 15:08:01                    2017-03-28   \n",
              "99437           2018-02-28 17:37:56                    2018-03-02   \n",
              "99438           2017-09-21 11:24:17                    2017-09-27   \n",
              "99439           2018-01-25 23:32:54                    2018-02-15   \n",
              "99440           2018-03-16 13:08:30                    2018-04-03   \n",
              "\n",
              "       sales_amount  \n",
              "0             29.99  \n",
              "1            118.70  \n",
              "2            159.90  \n",
              "3             45.00  \n",
              "4             19.90  \n",
              "...             ...  \n",
              "99436         72.00  \n",
              "99437        174.90  \n",
              "99438        205.99  \n",
              "99439        359.98  \n",
              "99440         68.50  \n",
              "\n",
              "[99441 rows x 9 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_data():\n",
        "    conn = duckdb.connect(\"brazilian_ecommerce.db\")\n",
        "    df = conn.execute(\"SELECT * FROM fact_orders\").fetchdf()\n",
        "    conn.close()\n",
        "    return df\n",
        "\n",
        "get_data()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
